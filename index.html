<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>PENG LI</title>
	<link rel="icon" type="image/x-icon" href="./favicon.ico"/>
	<style>
	  body {
		font-size: 18px;
	  }
	</style>
</head>

<body>
<p>
<font size="+2"><b>Peng Li (<font style="font-family:SimSun">李鹏</font>)</b></font> <br>
</p>

<p>
E-mail: lip21 [at] m.fudan.edu.cn; pengli.ds [at] gmail.com <br>
</p>

<p>
    <h3>Biography</h3>
</p>

<p>
Currently, Peng is a third-year M.S. student at Fudan University, advised by Prof. <a href="https://xpqiu.github.io/">Xipeng Qiu</a>.
Peng has been very fortunate to work with Prof. <a href="https://www.hongyuanmei.com/">Hongyuan Mei</a> and Prof. <a href="https://home.ttic.edu/~mwalter/">Matthew R. Walter</a> at <a href="https://www.ttic.edu/">TTIC</a>.
<br>
Before coming to Fudan, Peng earned his B.S. degree from the School of Data Science and Engineering, East China Normal University, working with Prof. <a href="http://ybwu.org/">Yuanbin Wu</a>.
</p>

<p>
Peng's research interests lie mainly in Foundation Models and Robot Learning. 
His research goal is to enable robots to interact with the world just like humans. 
Recently, Peng has been working on improving the tool learning ability of LLMs.
</p>

<p>
<h3>Publications (<a href="https://scholar.google.com/citations?user=4puDTtgAAAAJ&hl=en&oi=sra">Google Scholar</a>)</h3>
(*equal contribution)
</p>

<p>
<a href="https://arxiv.org/abs/2306.17840">Statler: State-Maintaining Language Models for Embodied Reasoning</a><br>
Takuma Yoneda*, Jiading Fang*, <b>Peng Li*</b>, Huanyu Zhang*, Tianchong Jiang, Shengjie Lin, Ben Picker, David Yunis, Hongyuan Mei, Matthew R. Walter<br>
<i>In submission</i><br>
</p>

<p>
<a href="https://github.com/OpenLMLab/MOSS">MOSS: Training Conversational Language Models from Synthetic Data</a><br>
Tianxiang Sun, Xiaotian Zhang, Zhengfu He, <b>Peng Li</b>, Qinyuan Cheng, Hang Yan, Xiangyang
Liu, Yunfan Shao, Qiong Tang, Xingjian Zhao, Ke Chen, Yining Zheng, Zhejian Zhou, Ruixiao
Li, Jun Zhan, Yunhua Zhou, Linyang Li, Xiaogui Yang, Lingling Wu, Zhangyue Yin, Xuanjing
Huang, Xipeng Qiu<br>
<i>To appear 2023</i><br>
</p>

<p>
<a href="https://arxiv.org/abs/2305.05711">CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors</a><br>
<b>Peng Li*</b>, Tianxiang Sun*, Qiong Tang, Hang Yan, Yuanbin Wu, Xuanjing Huang, Xipeng Qiu<br>
<i>ACL 2023</i><br> 
</p>


Last update: Aug. 2023
</body>
</html>
